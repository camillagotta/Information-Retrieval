{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Importing libraries\n",
    "Python imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading words: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import itertools\n",
    "import csv\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import lzma\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import xml.etree.cElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#!pip install -U spacy unidecode\n",
    "\n",
    "from unidecode import unidecode\n",
    "from spacy.matcher import Matcher \n",
    "from spacy import displacy \n",
    "from IPython.display import Image, display\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import skipgrams\n",
    "from nltk.corpus import wordnet as wn\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import FloatProgress\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#!pip install pyquery\n",
    "#!pip install -U gensim\n",
    "\n",
    "import gensim\n",
    "#import pyLDAvis.gensim\n",
    "import pickle\n",
    "from gensim import corpora\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading data\n",
    "The Dataset portion we are going to use for the project is the Illinois Bulk Dataset, composed by\n",
    "183146 CDs. Every CD can report one or more ‘opinions’, that are parts of the document where\n",
    "one or more judges express their argumentations and indeed, ‘opinions' on the real case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = \"https://case.law/download/bulk_exports/latest/by_jurisdiction/case_text_open/ill/ill_text.zip\"\n",
    "output_path = \"C:/Users/camilla.gotta/Downloads/Illinois-20200302-text.zip\"\n",
    "print(\"Downloading to %s ...\" % output_path)\n",
    "with open(output_path, 'wb') as out_file:\n",
    "    shutil.copyfileobj(requests.get(download_url, stream=True).raw, out_file)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The used dataset in this project is Harvard Caselaw Access Project. The Dataset portion we are going to use for the project is the Illinois Bulk Dataset, composed by 183146 CDs (https://case.law/bulk/download/). \n",
    "\n",
    "All CDs report the followings metadata: case body, data, attorneys, head_matter, judges, author, opinions (with the dissenting opinions where present), citations, court identification details, date,case ID, jurisdiction, names, reporter, volumes, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our bulk export is stored as a zipped folder in BagIt format, which in turn contains an zipped file in jsonlines format. \n",
    "#We can peel off all of those layers and get a list of cases with built-in Python libraries\n",
    "\n",
    "# a list to hold the cases we're sampling\n",
    "\n",
    "cases = []\n",
    "\n",
    "# decompress the file line by line     \n",
    "with zipfile.ZipFile('C:/Users/hadjiandrea/PycharmProjects/InformationRetrieval/ill_text_20200604.zip', 'r') as zip_archive:\n",
    "    xz_path = next(path for path in zip_archive.namelist() if path.endswith('/data.jsonl.xz'))\n",
    "    with zip_archive.open(xz_path) as xz_archive, lzma.open(xz_archive) as jsonlines:\n",
    "        for line in tqdm(jsonlines):\n",
    "            # decode the file into a convenient format\n",
    "            record = json.loads(str(line, 'utf-8'))\n",
    "            # if the decision date on the case matches one we're interested in, add to our list\n",
    "            if int(record['decision_date'][:4]):\n",
    "                cases.append(record)\n",
    "print(\"Number of Cases: {}\".format(len(cases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our bulk export is stored as a zipped folder in BagIt format, which in turn contains an zipped file in jsonlines format. \n",
    "#We can peel off all of those layers and get a list of cases with built-in Python libraries\n",
    "\n",
    "# a list to hold the cases we're sampling\n",
    "\n",
    "cases = []\n",
    "\n",
    "# decompress the file line by line     \n",
    "with zipfile.ZipFile('C:/Users/camilla.gotta/Downloads/Illinois-20200302-text.zip', 'r') as zip_archive:\n",
    "    xz_path = next(path for path in zip_archive.namelist() if path.endswith('/data.jsonl.xz'))\n",
    "    with zip_archive.open(xz_path) as xz_archive, lzma.open(xz_archive) as jsonlines:\n",
    "        for line in tqdm(jsonlines):\n",
    "            # decode the file into a convenient format\n",
    "            record = json.loads(str(line, 'utf-8'))\n",
    "            # if the decision date on the case matches one we're interested in, add to our list\n",
    "            if int(record['decision_date'][:4]):\n",
    "                cases.append(record)\n",
    "print(\"Number of Cases: {}\".format(len(cases)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be used:\n",
    "- `id`(assigned by CAP database): A unique case identifier that we can use to link opinions belonging to the same case\n",
    "- `name`: The case's name\n",
    "- `court`: The court in which the case was heard and decided\n",
    "- `citations`: The official citation to the case\n",
    "- `author`: The author of the opinion, if known\n",
    "- `type`: The type of the opinion (ex. 'majority,''dissent,''concurrence')\n",
    "- `text`: The full text of the opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a list comprehension to pull out the metadata attributes specified above\n",
    "opinion_data = []\n",
    "for case in cases:\n",
    "    for opinion in case[\"casebody\"][\"data\"][\"opinions\"]:\n",
    "        opinion_data.append({'id': case['id'],\n",
    "                             'year': case['decision_date'],\n",
    "                             'name': case['name'],\n",
    "                             'citation': case['citations'][0]['cite'],\n",
    "                             'court': case['court']['name'],\n",
    "                             'author': opinion['author'],\n",
    "                             'type': opinion['type'],\n",
    "                             'text': opinion['text'],})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_data = pd.DataFrame(opinion_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_data = opinion_data.apply(lambda x: x.str.lower() if x.dtype == 'object' else x)\n",
    "opinion_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"opinion_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(opinion_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload save pkl file\n",
    "with open('opinion_data.pkl', 'rb') as f:\n",
    "    opinion_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>citation</th>\n",
       "      <th>court</th>\n",
       "      <th>author</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1156845</td>\n",
       "      <td>1996-03-27</td>\n",
       "      <td>elsie ghere, widow of jim ghere, deceased, app...</td>\n",
       "      <td>278 ill. app. 3d 840</td>\n",
       "      <td>illinois appellate court</td>\n",
       "      <td>justice colwell</td>\n",
       "      <td>majority</td>\n",
       "      <td>justice colwell\\ndelivered the opinion of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1156786</td>\n",
       "      <td>1996-02-16</td>\n",
       "      <td>robert l. o'neil et al., plaintiffs-appellees ...</td>\n",
       "      <td>278 ill. app. 3d 327</td>\n",
       "      <td>illinois appellate court</td>\n",
       "      <td>presiding justice zwick</td>\n",
       "      <td>majority</td>\n",
       "      <td>presiding justice zwick\\ndelivered the opinion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1156800</td>\n",
       "      <td>1996-02-26</td>\n",
       "      <td>in re l.n., a minor (the people of the state o...</td>\n",
       "      <td>278 ill. app. 3d 46</td>\n",
       "      <td>illinois appellate court</td>\n",
       "      <td>justice slater</td>\n",
       "      <td>majority</td>\n",
       "      <td>justice slater\\ndelivered the opinion of the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1156841</td>\n",
       "      <td>1996-02-13</td>\n",
       "      <td>the people of the state of illinois, plaintiff...</td>\n",
       "      <td>278 ill. app. 3d 218</td>\n",
       "      <td>illinois appellate court</td>\n",
       "      <td>justice scariano</td>\n",
       "      <td>majority</td>\n",
       "      <td>justice scariano\\ndelivered the opinion of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1156843</td>\n",
       "      <td>1996-03-28</td>\n",
       "      <td>robert alan shramuk, plaintiff-appellant, v. c...</td>\n",
       "      <td>278 ill. app. 3d 745</td>\n",
       "      <td>illinois appellate court</td>\n",
       "      <td>justice geiger</td>\n",
       "      <td>majority</td>\n",
       "      <td>justice geiger\\ndelivered the opinion of the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197781</th>\n",
       "      <td>2866138</td>\n",
       "      <td>1967-05-18</td>\n",
       "      <td>raymond pedrick et al., appellants, vs. the pe...</td>\n",
       "      <td>37 ill. 2d 494</td>\n",
       "      <td>illinois supreme court</td>\n",
       "      <td>mr. justice underwood</td>\n",
       "      <td>majority</td>\n",
       "      <td>mr. justice underwood\\ndelivered the opinion o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197782</th>\n",
       "      <td>5550081</td>\n",
       "      <td>1988-02-11</td>\n",
       "      <td>the people of the state of illinois, appellee,...</td>\n",
       "      <td>122 ill. 2d 176</td>\n",
       "      <td>illinois supreme court</td>\n",
       "      <td>justice ryan</td>\n",
       "      <td>majority</td>\n",
       "      <td>justice ryan\\ndelivered the opinion of the cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197783</th>\n",
       "      <td>5550081</td>\n",
       "      <td>1988-02-11</td>\n",
       "      <td>the people of the state of illinois, appellee,...</td>\n",
       "      <td>122 ill. 2d 176</td>\n",
       "      <td>illinois supreme court</td>\n",
       "      <td>justice simon,</td>\n",
       "      <td>concurring-in-part-and-dissenting-in-part</td>\n",
       "      <td>justice simon,\\nconcurring in part and dissent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197784</th>\n",
       "      <td>5426498</td>\n",
       "      <td>1976-03-29</td>\n",
       "      <td>first capitol mortgage corporation, appellee, ...</td>\n",
       "      <td>63 ill. 2d 128</td>\n",
       "      <td>illinois supreme court</td>\n",
       "      <td>mr. justice ryan</td>\n",
       "      <td>majority</td>\n",
       "      <td>mr. justice ryan\\ndelivered the opinion of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197785</th>\n",
       "      <td>2882811</td>\n",
       "      <td>1965-11-19</td>\n",
       "      <td>the people of the state of illinois, defendant...</td>\n",
       "      <td>33 ill. 2d 417</td>\n",
       "      <td>illinois supreme court</td>\n",
       "      <td>mr. justice underwood</td>\n",
       "      <td>majority</td>\n",
       "      <td>mr. justice underwood\\ndelivered the opinion o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197786 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id        year  \\\n",
       "0       1156845  1996-03-27   \n",
       "1       1156786  1996-02-16   \n",
       "2       1156800  1996-02-26   \n",
       "3       1156841  1996-02-13   \n",
       "4       1156843  1996-03-28   \n",
       "...         ...         ...   \n",
       "197781  2866138  1967-05-18   \n",
       "197782  5550081  1988-02-11   \n",
       "197783  5550081  1988-02-11   \n",
       "197784  5426498  1976-03-29   \n",
       "197785  2882811  1965-11-19   \n",
       "\n",
       "                                                     name  \\\n",
       "0       elsie ghere, widow of jim ghere, deceased, app...   \n",
       "1       robert l. o'neil et al., plaintiffs-appellees ...   \n",
       "2       in re l.n., a minor (the people of the state o...   \n",
       "3       the people of the state of illinois, plaintiff...   \n",
       "4       robert alan shramuk, plaintiff-appellant, v. c...   \n",
       "...                                                   ...   \n",
       "197781  raymond pedrick et al., appellants, vs. the pe...   \n",
       "197782  the people of the state of illinois, appellee,...   \n",
       "197783  the people of the state of illinois, appellee,...   \n",
       "197784  first capitol mortgage corporation, appellee, ...   \n",
       "197785  the people of the state of illinois, defendant...   \n",
       "\n",
       "                    citation                     court  \\\n",
       "0       278 ill. app. 3d 840  illinois appellate court   \n",
       "1       278 ill. app. 3d 327  illinois appellate court   \n",
       "2        278 ill. app. 3d 46  illinois appellate court   \n",
       "3       278 ill. app. 3d 218  illinois appellate court   \n",
       "4       278 ill. app. 3d 745  illinois appellate court   \n",
       "...                      ...                       ...   \n",
       "197781        37 ill. 2d 494    illinois supreme court   \n",
       "197782       122 ill. 2d 176    illinois supreme court   \n",
       "197783       122 ill. 2d 176    illinois supreme court   \n",
       "197784        63 ill. 2d 128    illinois supreme court   \n",
       "197785        33 ill. 2d 417    illinois supreme court   \n",
       "\n",
       "                         author                                       type  \\\n",
       "0               justice colwell                                   majority   \n",
       "1       presiding justice zwick                                   majority   \n",
       "2                justice slater                                   majority   \n",
       "3              justice scariano                                   majority   \n",
       "4                justice geiger                                   majority   \n",
       "...                         ...                                        ...   \n",
       "197781    mr. justice underwood                                   majority   \n",
       "197782             justice ryan                                   majority   \n",
       "197783           justice simon,  concurring-in-part-and-dissenting-in-part   \n",
       "197784         mr. justice ryan                                   majority   \n",
       "197785    mr. justice underwood                                   majority   \n",
       "\n",
       "                                                     text  \n",
       "0       justice colwell\\ndelivered the opinion of the ...  \n",
       "1       presiding justice zwick\\ndelivered the opinion...  \n",
       "2       justice slater\\ndelivered the opinion of the c...  \n",
       "3       justice scariano\\ndelivered the opinion of the...  \n",
       "4       justice geiger\\ndelivered the opinion of the c...  \n",
       "...                                                   ...  \n",
       "197781  mr. justice underwood\\ndelivered the opinion o...  \n",
       "197782  justice ryan\\ndelivered the opinion of the cou...  \n",
       "197783  justice simon,\\nconcurring in part and dissent...  \n",
       "197784  mr. justice ryan\\ndelivered the opinion of the...  \n",
       "197785  mr. justice underwood\\ndelivered the opinion o...  \n",
       "\n",
       "[197786 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1771-10-12     1\n",
       "1819-12        5\n",
       "1820-07        6\n",
       "1820-12        6\n",
       "1822-12       16\n",
       "              ..\n",
       "2019-07-01     2\n",
       "2019-07-02     1\n",
       "2019-07-03     1\n",
       "2019-07-11     3\n",
       "2019-07-29     1\n",
       "Name: year, Length: 17958, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drange = pd.date_range('1850-01-01', '2020-01-01', freq='5Y')\n",
    "court_year = opinion_data['year'].value_counts().sort_index() #.plot.bar()\n",
    "#plt.savefig('court_year.png')\n",
    "court_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "court_lengths = opinion_data['court'].value_counts().sort_index() #.plot.bar()\n",
    "#plt.savefig('court_lengths.png')\n",
    "court_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "court_type = opinion_data['type'].value_counts().sort_index() #.plot.bar()\n",
    "#plt.savefig('court_type.png')\n",
    "court_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting and Cleaning Data\n",
    "\n",
    "The exact strategy you employ to clean data can be very dependent on the type of analysis you want to do in this one we will cover a few common strategies:\n",
    "\n",
    "removing non alphanumeric characters\n",
    "removing common stop words\n",
    "removing very short or long words\n",
    "removing purely numerical data\n",
    "removing words that are not in a particular corpus (such as the oxford english dictionary)\n",
    "\n",
    "Again, these are common techniques, but depending on your study and data, they may not be necessary or even advisable. The strategy you take will emerge from your data and the type of analysis you plan to do.\n",
    "\n",
    "### Removing non alphanumeric characters\n",
    "This loop uses the gemsin library to strip non alphanumeric characters. It also collapses multiple spaces into a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_texts = []\n",
    "for i in range(len(cases)):\n",
    "    if cases[i]['casebody']['data']['opinions']:\n",
    "        text = cases[i]['casebody']['data']['opinions'][0]['text'].lower()\n",
    "        text = strip_non_alphanum(text)\n",
    "        opinion_texts.append(' '.join(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       \n",
    "Here's the output from the first record. Take a look at this as compared to the text from the raw JSON file. Non alphanumeric characters are gone, but you can still read the text and understand what it means. However, you may already be able to see how cleaning data can result in loss of precision.\n",
    "\n",
    "As an exercise, take a look at the original document and consider if there are any terms you'd be interested in preserving that may be lost through the data transformation here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords\n",
    "The above text still contains a large number of \"stop words\", common words that may not too ubiquitous to be useful (emphasis on the may). Words like \"a\", \"they\", \"and\" increase the word count and require extra processing time and computing resources, and may not be helpful in your analysis.\n",
    "\n",
    "The loop below uses the gensim utility to remove common stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(opinion_texts)):\n",
    "    opinion_texts[i] = remove_stopwords(opinion_texts[i])\n",
    "#opinion_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove very long or short words, Remove purely numerical data\n",
    "The text still contains a large number of short or purely numeric characters. These may or may not contain valuable information - for now, we'll remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(opinion_texts)):\n",
    "    opinion_texts[i] = ' '.join([s for s in opinion_texts[i].split() if not (len(s) < 2 or s.isdigit() == True)])\n",
    "#opinion_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing words that are not in a particular corpus (such as the oxford english dictionary)\n",
    "\n",
    "You may have noticed that there are a number of words such as \"jj\" above that are nor part of the standard english language. These may be the result of data cleaning (such as artifacts of markup language). They may also be the result of legal language or other important data. As always, cleaning data is a decision you make that may or may not be necessary or desirable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "for i in range(len(opinion_texts)):\n",
    "    opinion_texts[i] = ' '.join([s for s in opinion_texts[i].split() if s in words])\n",
    "#opinion_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = set(nltk.word_tokenize(str(opinion_data['author'].tolist())))\n",
    "year = set(opinion_data['year'].tolist())\n",
    "name = set(nltk.word_tokenize(str(opinion_data['name'].tolist())))\n",
    "type_judge = set(opinion_data['type'].tolist())\n",
    "court = set(nltk.word_tokenize(str(opinion_data['court'].tolist())))\n",
    "citation = set(nltk.word_tokenize(str(opinion_data['citation'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#final = list(itertools.chain(authors, year, name, type_judge, court, citation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(opinion_texts)):\n",
    "               opinion_texts[i] = ' '.join([s for s in opinion_texts[i].split() if s not in authors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(opinion_texts)):\n",
    "               opinion_texts[i] = ' '.join([s for s in opinion_texts[i].split() if s not in year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(opinion_texts)):\n",
    "               opinion_texts[i] = ' '.join([s for s in opinion_texts[i].split() if s not in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(opinion_texts)):\n",
    "               opinion_texts[i] = ' '.join([s for s in opinion_texts[i].split() if s not in type_judge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(opinion_texts)):\n",
    "               opinion_texts[i] = ' '.join([s for s in opinion_texts[i].split() if s not in court])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(opinion_texts)):\n",
    "               opinion_texts[i] = ' '.join([s for s in opinion_texts[i].split() if s not in citation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save file to pkl\n",
    "with open(\"opinion_texts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(opinion_texts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenize the sentences\n",
    "We've cleaned the data, but it is still stored in large blocks of text. The next step, tokenization, will convert each line into a list of individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload save pkl file\n",
    "with open('opinion_texts.pkl', 'rb') as f:\n",
    "    opinion_texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_texts = pd.read_pickle(\"opinion_texts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_texts = pd.DataFrame(opinion_texts)\n",
    "opinion_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokenized_sentences = [] \n",
    "for text in opinion_texts:\n",
    "    tokenized_sentences.append(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_sentences[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenized_sentences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenized_sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenized_sentences.pkl', 'rb') as f:\n",
    "    tokenized_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun, adj\n",
    "data_lemmatized_2 = lemmatization(tokenized_sentences, allowed_postags=['NOUN', 'ADJ'])\n",
    "#print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_lemmatized_2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_lemmatized_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
